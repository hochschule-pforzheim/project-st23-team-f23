{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hochschule-pforzheim/project-st23-team-f23/blob/main/2_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datensatz vorbereiten - Dummies, Skalen"
      ],
      "metadata": {
        "id": "6ONQkrBSkKBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Im ersten Schritt wird der Datensatz aus dem ersten Notebook unter dem Namen 'maindf' geladen. Hier muss der Datensatz \"1_Cleansing_Join\" aus dem vorherigen Notebook reingeladen werden."
      ],
      "metadata": {
        "id": "24l74_e_fbQ5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Swltp5GDDCQQ",
        "outputId": "846e6e79-6481-4024-991c-74c3e2fd50f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#maindf = pd.read_csv(\"drive/MyDrive/Colab Notebooks/1_Cleansing_Join.csv\")\n",
        "#maindf = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AI & ML/Projekt/1_Cleansing_Join_final.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nach inhaltlicher Überlegung ist klar geworden, dass die Spalten 'arrival_date','departure_date', 'night_index' und 'arrival_date_week_number' für den Input von Klassifikationsmodellen irrelevant sind. Hiermit werden sie aus dem Dataframe gelöscht"
      ],
      "metadata": {
        "id": "atC4yrAWfqQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maindf = maindf.drop(columns=['arrival_date','departure_date', 'night_index', 'arrival_date_week_number'])"
      ],
      "metadata": {
        "id": "WnDW2pYk5mdR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maindf.isna().sum()"
      ],
      "metadata": {
        "id": "CWI_32CoH7bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Im folgenden Schritt werden bereits alle binären Variablen auch zu solch einem Datentyp geändert."
      ],
      "metadata": {
        "id": "1YUmrgpfgbl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maindf['is_canceled'] = maindf['is_canceled'].astype('uint8')\n",
        "maindf['is_repeated_guest'] = maindf['is_repeated_guest'].astype('uint8')\n",
        "maindf['holiday_flag'] = maindf['holiday_flag'].astype('uint8')\n",
        "maindf['agent'] = maindf['agent'].astype('uint8')\n"
      ],
      "metadata": {
        "id": "v3K_dgGPAZoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nun müssen alle nicht ordinalen, kategoriellen Features zu dummy-Variablen umgebaut werden."
      ],
      "metadata": {
        "id": "lJWSfe7Hgjyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maindf = pd.concat([\n",
        "    maindf.drop(columns=['city']), # remove the columns we one-hot-encode\n",
        "    pd.get_dummies(maindf.city, drop_first=True, prefix='city'),\n",
        "], axis=1) # join the one-hot-encoded columns by concatenating them side-by-side (axis 1)"
      ],
      "metadata": {
        "id": "yt5sdBTp7F5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maindf = pd.concat([\n",
        "    maindf.drop(columns=['distribution_channel']), # remove the columns we one-hot-encode\n",
        "    pd.get_dummies(maindf.distribution_channel, drop_first=True, prefix='distribution_channel'),\n",
        "], axis=1) # join the one-hot-encoded columns by concatenating them side-by-side (axis 1)"
      ],
      "metadata": {
        "id": "LJr9bN6HMPHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maindf = pd.concat([\n",
        "    maindf.drop(columns=['deposit_type']), # remove the columns we one-hot-encode\n",
        "    pd.get_dummies(maindf.deposit_type, drop_first=True, prefix='deposit_type'),\n",
        "], axis=1) # join the one-hot-encoded columns by concatenating them side-by-side (axis 1)"
      ],
      "metadata": {
        "id": "jQXPeYYfMdwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maindf = pd.concat([\n",
        "    maindf.drop(columns=['customer_type']), # remove the columns we one-hot-encode\n",
        "    pd.get_dummies(maindf.customer_type, drop_first=True, prefix='customer_type'),\n",
        "], axis=1) # join the one-hot-encoded columns by concatenating them side-by-side (axis 1)"
      ],
      "metadata": {
        "id": "k2UJLfvMM2so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maindf"
      ],
      "metadata": {
        "id": "ENDetLxvMZ6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Das Feature country entpuppt sich als Problem, da hier über 175 dummy variablen gebildet werden würden"
      ],
      "metadata": {
        "id": "7yugHrKsguyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(maindf.country.value_counts())"
      ],
      "metadata": {
        "id": "fQHBUpZ1YCr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wir sehen aber bereits bei folgender Ansicht dass sich die meisten Buchungen höchstwahrscheinlich auf einige wenige Länder fokussieren werden."
      ],
      "metadata": {
        "id": "vJij5D2LhJn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maindf.country.value_counts()"
      ],
      "metadata": {
        "id": "8vSglS7yhEHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maindf.groupby(['country']).sum()"
      ],
      "metadata": {
        "id": "vElvNsmFaYFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hier dann noch mal eine Ansicht aller Länder mit ihrer dazugehörigen Anzahl an Buchungen. Wir sehen dass relativ schnell schon keine vierstelligen Buchungsanzahlen mehr vorhanden sind. Aufgrundessen werden alle Länder aus dem Datensatz geworfen die n<1000 Buchungen aufweisen. Dieser modifizierte Dataframe wird maindf_c genannt"
      ],
      "metadata": {
        "id": "2oxJsAEbhTaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "country_counts = maindf['country'].value_counts()\n",
        "country_names = country_counts.index.tolist()\n",
        "counts = country_counts.tolist()\n",
        "\n",
        "result = list(zip(country_names, counts))\n",
        "print(result)"
      ],
      "metadata": {
        "id": "On3pzjCTggzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keep_countries = ['PRT', 'GBR', 'Fra', 'ESP', 'DEU', 'ITA', 'IRL', 'BEL', 'BRA', 'USA', 'NLD', 'CHE', 'CN', 'AUT', 'SWE']\n",
        "maindf_c = maindf[maindf['country'].isin(keep_countries)]"
      ],
      "metadata": {
        "id": "abMgxRKSmMs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mit folgenden 2 Zeilen sehen wir, dass gut 20000 Einträge durch diesen Schritt verloren gehen. Anschließend können jedoch Dummies gebildet werden und es sind anstatt 175 nur noch 14"
      ],
      "metadata": {
        "id": "KmbqzO2Xhwac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(maindf)"
      ],
      "metadata": {
        "id": "XjjNU51KMSCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(maindf_c)"
      ],
      "metadata": {
        "id": "bcTmGzOgm5f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maindf_c = pd.concat([\n",
        "    maindf_c.drop(columns=['country']), # remove the columns we one-hot-encode\n",
        "    pd.get_dummies(maindf_c.country, drop_first=True, prefix='country'),\n",
        "], axis=1) # join the one-hot-encoded columns by concatenating them side-by-side (axis 1)"
      ],
      "metadata": {
        "id": "UmtusnoXnSXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maindf_c.isna().sum()"
      ],
      "metadata": {
        "id": "zHPe-L_hnk3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maindf_c"
      ],
      "metadata": {
        "id": "n9T_CFC9B1Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Da der 'arrival_date_month' noch in einem String vorliegt, werden die Werte nun durch die Monatszahlen ausgetauscht um eine bessere Verarbeitung durch das spätere Modell zu gewährleisten."
      ],
      "metadata": {
        "id": "K3AD-03MiAOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arrival_date_month_mapping = {\n",
        "    'January': 1,\n",
        "    'February': 2,\n",
        "    'March': 3,\n",
        "    'April': 4,\n",
        "    'May': 5,\n",
        "    'June': 6,\n",
        "    'July': 7,\n",
        "    'August': 8,\n",
        "    'September': 9,\n",
        "    'October': 10,\n",
        "    'November': 11,\n",
        "    'December': 12\n",
        "}\n",
        "maindf_c.arrival_date_month = maindf_c.arrival_date_month.apply(lambda x: arrival_date_month_mapping[x])"
      ],
      "metadata": {
        "id": "Z6Rcb3aPC19c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "wir sehen dass alle features in dem gewünschten Datenformat vorliegen, nur children erklärt sich uns nicht ganz."
      ],
      "metadata": {
        "id": "mshf_VqTintE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maindf_c.dtypes"
      ],
      "metadata": {
        "id": "djCPuTq38fCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Da, wie hier zu sehen lediglich integer-Werte in der 'children' Spalte vorhanden sind, wird das Datenformat auch zu diesem geändert."
      ],
      "metadata": {
        "id": "WcOMG107i1La"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maindf_c.children.value_counts()"
      ],
      "metadata": {
        "id": "zGNia7lhOHK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maindf_c['children'] = maindf_c['children'].astype('int64')\n"
      ],
      "metadata": {
        "id": "cLI2BUhsOpt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zudem werden zwei dummies umbenannt, damit später keine Probleme durch dessen Namen aufkommen. Bei beiden sind Satzzeichen vorhanden, mit denen Python in gewissen Codeausführungen nicht umgehen kann. Es wird das Leerzeichen, sowie das slash zu einem Unterstrich geändert. Somit haben wir bei allen features einen zusammenhängenden string."
      ],
      "metadata": {
        "id": "BZf_PWFIjGsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maindf_c= maindf_c.rename(columns={'deposit_type_Non Refund': 'deposit_type_Non_Refund'})\n"
      ],
      "metadata": {
        "id": "VCd8w-OiUAaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maindf_c= maindf_c.rename(columns={'distribution_channel_TA/TO': 'distribution_channel_TA_TO'})\n"
      ],
      "metadata": {
        "id": "3nLKvemVUyiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maindf_c.isna().sum()"
      ],
      "metadata": {
        "id": "bD5jbf-R0qDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wenn wir uns die durchschnittlichen Features, gruppiert nach den Ausprägungen unserer Zielvariablen anschauen, kann man schon erste kleine Ableitungen erkennen. Beispielsweise ist im Schnitt die Leadtime deutlich höher bei den Buchungen, welche gecancelt werden."
      ],
      "metadata": {
        "id": "70RWsLl4jebr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maindf_c.groupby('is_canceled').mean()"
      ],
      "metadata": {
        "id": "IWeB6zqELx5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Um Multikollinearität ausschließen zu können wird der VIF-Faktor für die Features berechnet."
      ],
      "metadata": {
        "id": "2sFAC2E_j6IP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from patsy import dmatrices\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "#find design matrix for regression model using 'rating' as response variable\n",
        "yy, XX = dmatrices('is_canceled ~ lead_time+arrival_date_year+arrival_date_month+arrival_date_day_of_month+stays_in_weekend_nights+stays_in_week_nights+stay_nights_sum+adults+children+babies+is_repeated_guest+previous_cancellations+previous_bookings_not_canceled+booking_changes+agent+days_in_waiting_list+adr+total_of_special_requests+average_temperature+average_temp_min+average_temp_max+average_prcp+holiday_flag+city_Lisbon+distribution_channel_Direct+distribution_channel_GDS+distribution_channel_TA_TO+distribution_channel_Undefined+deposit_type_Non_Refund+deposit_type_Refundable+customer_type_Group+customer_type_Transient+customer_type_Transient-Party+country_BEL+country_BRA+country_CHE+country_CN+country_DEU+country_ESP+country_GBR+country_IRL+country_ITA+country_NLD+country_PRT+country_SWE+country_USA', data=maindf_c, return_type='dataframe')\n",
        "\n",
        "#create DataFrame to hold VIF values\n",
        "vif_df = pd.DataFrame()\n",
        "vif_df['variable'] = XX.columns\n",
        "\n",
        "#calculate VIF for each predictor variable\n",
        "vif_df['VIF'] = [variance_inflation_factor(XX.values, i) for i in range(XX.shape[1])]\n",
        "\n",
        "#view VIF for each predictor variable\n",
        "print(vif_df)"
      ],
      "metadata": {
        "id": "mWZzdQmyRKbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prüfung auf Multikollinearität"
      ],
      "metadata": {
        "id": "t0fkIITNke8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nun wird der VIF-Wert in ein anderes Format gebracht, um eine einfachere Interpretation zu gewährleisten. Vor allem die Features, welche die Temperatur abbilden zeigen einen extrem hohen VIF-Wert, was auch vollkommen logisch ist. Sie sind stark voneinander abhängig. Darum wird entschieden für die Temperatur lediglich die 'average_temperature' zu behalten."
      ],
      "metadata": {
        "id": "xUoz-cMwkGZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funktion zur Formatierung der Zahlen\n",
        "def format_number(num):\n",
        "    return \"{:.8f}\".format(num)\n",
        "\n",
        "# Anwendung der Funktion auf die Spalte 'Zahlen'\n",
        "vif_df['VIF'] = vif_df['VIF'].apply(format_number)\n",
        "\n",
        "# Ausgabe des formatierten DataFrames\n",
        "print(vif_df)"
      ],
      "metadata": {
        "id": "G_AaKdyvVp3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maindf_c = maindf_c.drop(columns=['average_temp_min', 'average_temp_max'])"
      ],
      "metadata": {
        "id": "fX8Ye92dYwyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maindf_c = maindf_c.drop(columns=['stays_in_week_nights', 'stays_in_weekend_nights'])"
      ],
      "metadata": {
        "id": "VYYdLxeo149J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hier wird der vorbereitete Datensatz als csv-Datei heruntergeladen, damit im nächsten Notebook für die Regression darauf zurückgegriffen werden kann."
      ],
      "metadata": {
        "id": "BEQWaSafhmV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maindf_c.to_csv('2_Data_for_Modeltraining.csv', header=True, index=False)"
      ],
      "metadata": {
        "id": "FtZ5Vu7dhEdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skalieren und Explorieren des Datensatzes"
      ],
      "metadata": {
        "id": "EsdzAkO1kuXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "maindf_scaled = maindf_c.copy()\n",
        "columns_to_scale = list(set(maindf_c.columns) - {'is_canceled'}) # take all columns except Loan_Status_Y\n",
        "maindf_scaled[columns_to_scale] = scaler.fit_transform(maindf_c[columns_to_scale])"
      ],
      "metadata": {
        "id": "JTw0a77KEHCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_columns = ['is_canceled','arrival_date_year', 'arrival_date_month', 'arrival_date_day_of_month', 'agent', 'is_repeated_guest', 'holiday_flag', 'city_Lisbon', 'distribution_channel_Direct', 'distribution_channel_GDS', 'distribution_channel_Undefined', 'distribution_channel_TA_TO', 'deposit_type_Non_Refund', 'deposit_type_Refundable', 'customer_type_Group', 'customer_type_Transient', 'customer_type_Transient-Party',\n",
        "       'country_BEL', 'country_BRA', 'country_CHE', 'country_CN', 'country_DEU', 'country_ESP', 'country_GBR', 'country_IRL', 'country_ITA', 'country_NLD', 'country_PRT', 'country_SWE', 'country_USA']\n",
        "numerical_columns = list(set(maindf_c.columns) - set(encoded_columns))"
      ],
      "metadata": {
        "id": "K-5CahAZEoto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for column in encoded_columns:\n",
        "  fig = plt.figure() # required if we want to plot multiple figures from the same cell\n",
        "  maindf_c[column].value_counts(sort=False).sort_index().plot(kind='bar', title=column)"
      ],
      "metadata": {
        "id": "Nv5JMYY4FvDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in numerical_columns:\n",
        "  maindf_c.plot(kind='hist', y=column)"
      ],
      "metadata": {
        "id": "01Qxjxc-GVSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "# .corr() calculates the correlation between each column in our dataframe\n",
        "sns.heatmap(maindf_c.corr(), annot=False, cmap='coolwarm')"
      ],
      "metadata": {
        "id": "G30unu9yGph1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = maindf_c.drop(columns='is_canceled')\n",
        "y = maindf_c.is_canceled"
      ],
      "metadata": {
        "id": "QPmMizR7TlpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_scaled = maindf_scaled.drop(columns='is_canceled')\n",
        "y_scaled = maindf_scaled.is_canceled"
      ],
      "metadata": {
        "id": "HZ9dxSYZTPsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdtTlRCngCXz"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y, shuffle = True)\n",
        "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42, stratify = y_scaled, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install imbalanced-learn"
      ],
      "metadata": {
        "id": "P5-KqVDaOzJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_smote, y_smote = smote.fit_resample(X_train,y_train)\n",
        "X_scaled_smote, y_scaled_smote = smote.fit_resample(X_train_scaled,y_train_scaled)"
      ],
      "metadata": {
        "id": "QnKALAjnSU8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = X_train.columns\n",
        "\n",
        "X_scaled_smote = pd.DataFrame(data=X_scaled_smote, columns=columns)\n",
        "y_scaled_smote = pd.DataFrame(data=y_scaled_smote, columns=['is_canceled'])\n",
        "\n",
        "print(\"length of oversampled data is \",len(X_scaled_smote))\n",
        "print(\"Number of no cancellation in oversampled data\",len(y_scaled_smote[y_scaled_smote['is_canceled']==0]))\n",
        "print(\"Number of cancellation\",len(y_scaled_smote[y_scaled_smote['is_canceled']==1]))"
      ],
      "metadata": {
        "id": "RoeoMtFpU8l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "def evaluate(clf, X_test_scaled, y_true):\n",
        "  y_pred = clf.predict(X_test_scaled)\n",
        "  print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "id": "6p_myojOHNZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regressionsmodelle"
      ],
      "metadata": {
        "id": "TM5bYgRgk-sr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistische Regression"
      ],
      "metadata": {
        "id": "uqbxTVUHlDHl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvsTRWzSn91A"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSuGgZnDn-Bt"
      },
      "source": [
        "logistic = LogisticRegression(random_state=56)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd3KWi8JoBml"
      },
      "source": [
        "logistic.fit(X_scaled_smote, y_scaled_smote)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "rfe = RFE(estimator=logistic)#, n_features_to_select=10)\n",
        "\n",
        "rfe.fit(X_scaled_smote, y_scaled_smote)\n",
        "\n",
        "\n",
        "selected_features = pd.DataFrame({'Feature': X_scaled_smote.columns, 'Selected': rfe.support_, 'Ranking': rfe.ranking_})\n",
        "print(selected_features)"
      ],
      "metadata": {
        "id": "MopEg0g4YlOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ranking = pd.DataFrame({'Feature': X_scaled_smote.columns, 'Ranking': rfe.ranking_})\n",
        "ranking = ranking.sort_values('Ranking')\n",
        "print(ranking)"
      ],
      "metadata": {
        "id": "IxziddKiadyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOlZ0QBVoDsy"
      },
      "source": [
        "evaluate(logistic, X_test_scaled, y_test_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic.score(X_test_scaled, y_test_scaled)"
      ],
      "metadata": {
        "id": "TY67dVFLFTTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explorieren der Predictions"
      ],
      "metadata": {
        "id": "11PDOqULl6EL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "ConfusionMatrixDisplay.from_estimator(\n",
        "    logistic, X_test_scaled, y_test_scaled, labels=[0, 1], display_labels=['Booking cancelled', 'Booking not canceled'])"
      ],
      "metadata": {
        "id": "NLy_NRCeGr1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = logistic.predict(X_test_scaled)\n",
        "df_with_predictions = pd.concat([X_test, y_test], axis='columns')\n",
        "df_with_predictions = pd.concat([df_with_predictions, pd.DataFrame(predictions, columns=['Prediction'], index=df_with_predictions.index)], axis=1)"
      ],
      "metadata": {
        "id": "8MHV8KgkI0YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "misclassified_df = df_with_predictions[df_with_predictions.is_canceled != df_with_predictions.Prediction]"
      ],
      "metadata": {
        "id": "_tohLj59JF1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "misclassified_df"
      ],
      "metadata": {
        "id": "fI_BamffJLHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in encoded_columns:\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2) # required if we want to plot multiple figures from the same cell\n",
        "  df_with_predictions[column].value_counts(sort=False).sort_index().plot(kind='bar', title=f'{column} (All)', ax=ax1)\n",
        "  misclassified_df[column].value_counts(sort=False).sort_index().plot(kind='bar', title=f'{column} (Misclassified)', ax=ax2)"
      ],
      "metadata": {
        "id": "w66nUnIbJXpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in numerical_columns:\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\n",
        "  df_with_predictions.plot(kind='hist', y=column, title=f'{column} (All)', ax=ax1)\n",
        "  misclassified_df.plot(kind='hist', y=column, title=f'{column} (Misclassified)', ax=ax2)"
      ],
      "metadata": {
        "id": "h6BHgp6JJvg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "logistic_roc_auc = roc_auc_score(y_test, logistic.predict(X_test_scaled))\n",
        "fpr, tpr, thresholds = roc_curve(y_test, logistic.predict_proba(X_test_scaled)[:,1])\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logistic_roc_auc)\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('Log_ROC')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ePuvtpAg_zRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Importance und Variablen-Reduktion"
      ],
      "metadata": {
        "id": "mXcP5Pm2mQtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importance\n",
        "feature_importance = abs(logistic.coef_[0])\n",
        "\n",
        "# Create a DataFrame with feature importance\n",
        "importance_df = pd.DataFrame({'Feature': X_scaled_smote.columns, 'Importance': feature_importance})\n",
        "\n",
        "# Sort the DataFrame by importance\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "print(importance_df)"
      ],
      "metadata": {
        "id": "QiiI0CAz7SeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "less_features_df = maindf_c.drop(columns=['average_prcp', 'babies', 'agent', 'days_in_waiting_list', 'children', 'holiday_flag', 'stay_nights_sum', 'adults', 'average_temperature', 'customer_type_Group', 'distribution_channel_Undefined', 'country_CN', 'deposit_type_Refundable', 'country_SWE', 'country_NLD', 'country_BEL', 'distribution_channel_GDS', 'country_DEU', 'country_CHE', 'customer_type_Transient-Party', 'country_GBR', 'country_USA'])"
      ],
      "metadata": {
        "id": "GbipnTrGBWcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_X = less_features_df.drop(columns='is_canceled')\n",
        "new_y = less_features_df.is_canceled"
      ],
      "metadata": {
        "id": "4OmtgeCBChVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_X_scaled = less_features_df.drop(columns='is_canceled')\n",
        "new_y_scaled = less_features_df.is_canceled"
      ],
      "metadata": {
        "id": "oGeNUDglCmoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_X_train, new_X_test, new_y_train, new_y_test = train_test_split(new_X, new_y, test_size=0.2, random_state=42, stratify = new_y, shuffle = True)\n",
        "new_X_train_scaled, new_X_test_scaled, new_y_train_scaled, new_y_test_scaled = train_test_split(new_X_scaled, new_y_scaled, test_size=0.2, random_state=42, stratify = new_y_scaled, shuffle = True)"
      ],
      "metadata": {
        "id": "K1EbbdPUCtGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_smote = SMOTE(random_state=42)\n",
        "new_X_smote, new_y_smote = smote.fit_resample(new_X_train,new_y_train)\n",
        "new_X_scaled_smote, new_y_scaled_smote = smote.fit_resample(new_X_train_scaled,new_y_train_scaled)"
      ],
      "metadata": {
        "id": "PQsZEYlMDSd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def new_evaluate(clf, new_X_test, new_y_true):\n",
        "  new_y_pred = clf.predict(new_X_test)\n",
        "  print(classification_report(new_y_true, new_y_pred))"
      ],
      "metadata": {
        "id": "2jJVghkNDmtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_logistic = LogisticRegression(random_state=56)"
      ],
      "metadata": {
        "id": "9vbZCg6sHxj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_logistic.fit(new_X_scaled_smote, new_y_scaled_smote)"
      ],
      "metadata": {
        "id": "mhwfLg8CD2lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_evaluate(new_logistic, new_X_test_scaled, new_y_test_scaled)"
      ],
      "metadata": {
        "id": "e2AGRvWDD-6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_logistic.score(new_X_test_scaled, new_y_test_scaled)"
      ],
      "metadata": {
        "id": "rXTDFm7kEQOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparametertuning"
      ],
      "metadata": {
        "id": "xZvyBVaTnO2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import loguniform\n",
        "\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'C': [0.1, 1, 10],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}"
      ],
      "metadata": {
        "id": "oC8Fw9GTKlIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "grid_search = GridSearchCV(logistic, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the grid search to your data\n",
        "grid_search.fit(X_scaled, y_scaled)\n",
        "\n",
        "# Get the best hyperparameters and the best model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Evaluate the best model on your data\n",
        "y_pred = best_model.predict(X_scaled)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_scaled, y_pred))\n"
      ],
      "metadata": {
        "id": "TGWAwzO5LYtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hyper_evaluate(clf, X_test, y_true):\n",
        "  hyper_y_pred = clf.predict(X_test)\n",
        "  print(classification_report(y_true, hyper_y_pred))"
      ],
      "metadata": {
        "id": "c-MA4c3b0Mf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mit dem Parameter l2 wird die sogenannte L2-Regulierung angewandt und verringert so die Gefahr vor Overfitting stark, da die Komplexität des Modells reduziert wird und somit weniger anfällig für overfitting ist. Das Modell wird bestraft wenn es große Koeffizienten verwendet. Die Koeffizienten werden während des Trainingsprozesses kleiner gehalten."
      ],
      "metadata": {
        "id": "Q_47MQJH0lcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyper_logistic = LogisticRegression(random_state=56, C=0.1, penalty='l2', solver='liblinear')"
      ],
      "metadata": {
        "id": "DMd9R9ZRzdJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyper_logistic.fit(X_scaled_smote, y_scaled_smote)"
      ],
      "metadata": {
        "id": "qZGuE49wzdJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyper_evaluate(hyper_logistic, X_test_scaled, y_test_scaled)"
      ],
      "metadata": {
        "id": "BItiYKZ8zdJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN"
      ],
      "metadata": {
        "id": "dCN6vGcxtFdx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47YLYPXKrEFX"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGszH4GtyQKe"
      },
      "source": [
        "knn = KNeighborsClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv9ogIsXycD0"
      },
      "source": [
        "knn.fit(X_train_scaled, y_train_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = knn.predict(X_test_scaled)\n",
        "\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test_scaled, y_pred))"
      ],
      "metadata": {
        "id": "yZOt4ila7mNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpRiNBtw_rrm"
      },
      "source": [
        "evaluate(knn, X_test_scaled, y_test_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JTD-7tftRa3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "ConfusionMatrixDisplay.from_estimator(\n",
        "    knn, X_test_scaled, y_test_scaled, labels=[0, 1], display_labels=['Booking cancelled', 'Booking not canceled'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparametertuning"
      ],
      "metadata": {
        "id": "yluEGrJrtZ-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "ht = RandomizedSearchCV(knn, param_grid, random_state=42, n_iter=10)"
      ],
      "metadata": {
        "id": "w0ug7_Q4u8qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "4ZDefvmUgzOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_params = { 'n_neighbors' : [5,7,9,11,13,15],\n",
        "               'weights' : ['uniform','distance'],\n",
        "               'metric' : ['minkowski','euclidean','manhattan']}"
      ],
      "metadata": {
        "id": "cUK8xZ46hSxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gs = GridSearchCV(KNeighborsClassifier(), grid_params, verbose = 1, cv=3, n_jobs = -1)"
      ],
      "metadata": {
        "id": "17EyPS7uhrCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the model on our train set\n",
        "g_res = gs.fit(X_train_scaled, y_train_scaled)"
      ],
      "metadata": {
        "id": "Cl0VPYBVhyip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the best score\n",
        "g_res.best_score_"
      ],
      "metadata": {
        "id": "Y_8iuQNTonth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the hyperparameters with the best score\n",
        "ht_params=g_res.best_params_\n",
        "ht_params"
      ],
      "metadata": {
        "id": "eHTT2Gk2opn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the best hyperparameters\n",
        "knn = KNeighborsClassifier(n_neighbors = 5, weights = 'uniform',algorithm = 'brute',metric = 'minkowski')\n",
        "knn.fit(X_train_scaled, y_train_scaled)"
      ],
      "metadata": {
        "id": "AHeyuSbGouux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get a prediction\n",
        "y_hat = knn.predict(X_train_scaled)\n",
        "y_knn = knn.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "TeT8WSPKo4tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(g_res, X_test_scaled, y_test_scaled)"
      ],
      "metadata": {
        "id": "ISWAG44ipG7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kqh8_e2sbaMv"
      },
      "source": [
        "# **Support Vector Machines**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0USwAAN1beq1"
      },
      "source": [
        "https://www.inovex.de/de/blog/support-vector-machines-guide/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7HxCuHngDxS"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/svm.html\n",
        "https://dataaspirant.com/svm-kernels/#t-1608140512020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACCodxyDmzxq"
      },
      "source": [
        "SVC gar nicht gut für große Samples https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html, herausgefunden, nachdem lange gedauert und abgebrochen."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machines sind im Machine Learning weit verbreitete Algorithmen, insbesondere für Klassifikationsaufgaben. Dabei kann über den Parameter 'kernel' bestimmt werden, durch welche Funktionsform die Klassen gebildet werden (linear, rbf, polynomial). Für große Datenmengen, wie sie in unserem Fall vorliegen, eignen sich die Standard-SVCs leider nicht, wir benötigen die schnellere Implementierung, den LinearSVC. Dieser Classifier akzeptiert allerdings den Parameter 'kernel' nicht bzw. der Parameter muss nicht definiert werden.\n",
        "\n",
        "Wir importieren den LinearSVC aus dem sklearn.svm-package, trainieren das Modell und evaluieren das gefittete Modell."
      ],
      "metadata": {
        "id": "d1am2DjqKhEo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9VYYOhRqHLS"
      },
      "outputs": [],
      "source": [
        "# for LinearSVC\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# for predicting labels\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "\n",
        "#for Hyperparameter Tuning\n",
        "from scipy.stats import loguniform\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "#for Gradient Boosting Classifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19qPovizcJkw"
      },
      "outputs": [],
      "source": [
        "X, y = make_classification(n_features=4, random_state=42)\n",
        "svc_clf = LinearSVC(random_state=42, tol=1e-5)\n",
        "\n",
        "svc = svc_clf.fit(X_train_scaled, y_train_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FSx2a1Tmkqz"
      },
      "outputs": [],
      "source": [
        "y_svc_predict = svc_clf.predict(X_test_scaled)\n",
        "y_svc_predict = y_svc_predict.reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1K5C7dPcPaI"
      },
      "outputs": [],
      "source": [
        "evaluate(svc, X_test_scaled,y_test_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Über die Confusion Matrix können wir nun darstellen, wie häufig bestimmte Werte predictet wurden und stellen diese den tatsächlich eingetretenen Fällen gegnüber."
      ],
      "metadata": {
        "id": "_SpaPN06MleW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0kY7Ko5CqpJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_scaled, y_svc_predict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3XiHzR1CqpK"
      },
      "source": [
        "> true positive (TP) predictions\n",
        "\n",
        "> true negative (TN) predictions\n",
        "\n",
        "> false positive (FP) predictions\n",
        "\n",
        "> false negative (FN) predictions\n",
        "\n",
        "> The decision tree **correctly** predicted 11,617 instances as negative (TN) and 6,703 instances as positive (TP).\n",
        "\n",
        "> The decision tree made 2,101 **false** positive predictions (FP) and 1,862 false negative predictions (FN)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noch übersichtlicher werden die Ergebnisse der Confusion Matrix in einem Plot."
      ],
      "metadata": {
        "id": "_hEBsIylMzxM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJvPqbDTCqpK"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "ConfusionMatrixDisplay.from_estimator(\n",
        "    svc, X_test_scaled, y_test_scaled, labels=[0, 1], display_labels=['Not canceled', 'Canceled'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYQU5zYspx5z"
      },
      "source": [
        "## **Hyperparameter Tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZKT2XixsMTs"
      },
      "source": [
        "C: da LinearSVC: andere Parameter wie 'learning_rate' nicht valid.\n",
        "\n",
        "Die Fehlermeldung \"TypeError: Parameter grid for parameter 'C' is not iterable or a distribution (value=1)\" deutet darauf hin, dass es ein Problem mit dem angegebenen Parameterraum für den Parameter 'C' gibt. Der Wert '1' allein ist nicht als Parameterraum gültig.\n",
        "\n",
        "Um das Problem zu beheben, kannst du entweder einen spezifischen Wert oder einen Parameterraum für 'C' angeben. Hier sind zwei Möglichkeiten:\n",
        "\n",
        "from scipy.stats import uniform\n",
        "\n",
        "param_grid = {\n",
        "    'C': uniform(loc=0.1, scale=10),\n",
        "\n",
        "    Die Wahl zwischen der Verwendung spezifischer Werte und der Verwendung eines Parameterraums hat Vor- und Nachteile, die ich im Folgenden erläutern werde:\n",
        "\n",
        "Option 1: Spezifische Werte für 'C':\n",
        "Vorteile:\n",
        "\n",
        "Einfachheit: Die Verwendung spezifischer Werte ist einfach und erfordert keine weiteren Berechnungen.\n",
        "Kontrolle: Du hast direkte Kontrolle über die ausgewählten Werte für 'C' und kannst gezielt bestimmte Werte testen.\n",
        "Nachteile:\n",
        "\n",
        "Möglicher Informationsverlust: Es besteht die Möglichkeit, dass der optimale Wert für 'C' nicht in den spezifischen Werten enthalten ist. Dies kann dazu führen, dass das Tuning nicht das bestmögliche Ergebnis erzielt.\n",
        "Option 2: Parameterraum für 'C':\n",
        "Vorteile:\n",
        "\n",
        "Flexibilität: Durch die Verwendung eines Parameterraums kannst du einen breiteren Bereich von 'C'-Werten abdecken und somit eine größere Chance haben, den optimalen Wert zu finden.\n",
        "Automatisierte Suche: Der Hyperparameter-Tuning-Algorithmus (z. B. RandomizedSearchCV) kann den Parameterraum automatisch durchsuchen und die besten Kombinationen finden.\n",
        "Nachteile:\n",
        "\n",
        "Komplexität: Die Definition eines Parameterraums erfordert möglicherweise zusätzliche Überlegungen und Kenntnisse über die Wertebereiche von 'C'. Die Auswahl eines geeigneten Parameterraums kann eine gewisse Erfahrung oder Experimentation erfordern.\n",
        "Berechnungsaufwand: Ein breiterer Parameterraum kann zu einer erhöhten Rechenzeit führen, da der Hyperparameter-Tuning-Algorithmus mehr Kombinationen ausprobieren muss.\n",
        "Es ist wichtig zu beachten, dass die Vor- und Nachteile von Optionen wie spezifischen Werten oder Parameterräumen stark vom Anwendungsfall und den Eigenschaften des Modells abhängen können. Es empfiehlt sich, verschiedene Optionen auszuprobieren und die Auswirkungen auf die Leistung des Modells zu beobachten, um die besten Hyperparameter-Tuning-Strategien zu ermitteln.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owJHJYT8pwWd"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'class_weight': [None, 'balanced'],\n",
        "    'dual': [True, False],\n",
        "    'fit_intercept': [True, False],\n",
        "    'intercept_scaling': loguniform(0.1, 10),\n",
        "    'loss': ['hinge', 'squared_hinge'],\n",
        "    'max_iter': [100, 500, 1000],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'tol': loguniform(0.0001, 0.1),\n",
        "    'verbose': [0, 1]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok21JGPjpwWe"
      },
      "outputs": [],
      "source": [
        "ht_svc = RandomizedSearchCV(svc_clf, param_grid, random_state=42, n_iter=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMRIg-zqpwWe"
      },
      "outputs": [],
      "source": [
        "ht_svc.fit(X_train_scaled, y_train_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmS_PT2Oum7D"
      },
      "source": [
        "Trotz Hyperparameter Tuning richtig schlechte Accuracy => nicht geeignet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pi9mStY_7-0H"
      },
      "outputs": [],
      "source": [
        "ht_svc.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "728TuCjapwWe"
      },
      "outputs": [],
      "source": [
        "evaluate(ht_svc, X_test_scaled, y_test_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX0wfX1ykSz7"
      },
      "source": [
        "# **Gradient Boosting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u5RRQKH0yRn"
      },
      "source": [
        "\n",
        "\n",
        "> Als nächstes Modell testen wir das additive Ensemble-Modell 'Gradient Boosting'. Hierbei werden schwächere Lernmodelle, i. d. R. Entscheindungsbäume schrittweise hinzugefügt und somit zu einem starken Lernmodell kombiniert. Im Gegensatz zum Random Forest werden beim Gradient Boosting gezielt schwache, fehlerhafte Lernmodelle genutzt, um diese zu kombinieren und Schwachstellen auszugleichen. Schritt für Schritt wird ein neues schwaches Lernmodell hinzugezogen und die Fehler korrigiert.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> Für Gradient Boosting benötigen wir den GradientBoostingClassifier. Der Parameter 'n_estimators' des Classifiers beschreibt dabei die Anzahl an schwächeren Lernmodellen, die sukzessive ins Trainingsmodell mitaufgenommen werden. DIe 'learning_rate' gibt den Hyperparameter sn, durch den Overfitting vermieden werden soll, der also das gesamte Modell reguliert. die Größe des decision trees wird durch 'max_depth' definiert. Über den Hyperparameter 'random_state' wird bestimmt, dass train- und test-Daten immer die selben Werte annehmen und somit das Machine Learning Modell immer die gleichen Ergebnisse liefert.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk6B-Fx8SpKX"
      },
      "source": [
        "\n",
        "> Um GradientBossting als Classification durchführen zu können, müssen wir erstmal den gradientBoostingClassifier aus sklearn.ensemble importieren.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipJ3X5Ry-iJh"
      },
      "outputs": [],
      "source": [
        "from numpy import loadtxt\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import plot_tree\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wir bestimmen die Parameter n_estimators, learning_rate und max_depth entsprechend der Python-Dokumentation und bestimmen einen random_state, um immer gleiche Ergebnisse mit Training und Evaluation zu erhalten."
      ],
      "metadata": {
        "id": "tCpPz0wZM_Ok"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKfwrFL3kZ6a"
      },
      "outputs": [],
      "source": [
        "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGLDgoQukeRI"
      },
      "outputs": [],
      "source": [
        "gb.fit(X_train_scaled, y_train_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Das trainierte Modell kann evaluiert werden. Wir sehen, dass die Accuracy bei 82% liegt, der f1-Score liegt knapp darunter, bei %."
      ],
      "metadata": {
        "id": "JSLNqMVWNNlC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8MNbk8bkh-n"
      },
      "outputs": [],
      "source": [
        "evaluate(gb, X_test_scaled, y_test_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWCzglM9k1GF"
      },
      "source": [
        "## **Hyperparameter Tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT536CMHlHMd"
      },
      "source": [
        "AUS SKRIPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qVTsJxtk5Xg"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'loss': ['log_loss', 'exponential'],\n",
        "    'learning_rate': loguniform(0.001, 1),\n",
        "    'n_estimators': [50, 100, 200, 220, 250],\n",
        "    'subsample': [0.3, 0.5, 0.8, 1.0],\n",
        "    'max_depth': [3, 5, 7, 11],\n",
        "    'min_samples_split': [1, 2, 5, 10, 15],\n",
        "    'min_samples_leaf': [1, 2, 4, 6],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnwyRwR0HXkW"
      },
      "source": [
        "https://colab.research.google.com/drive/1alinCu_hm_69JQRA_jPoCB_PY3xnE7Br#scrollTo=2qVTsJxtk5Xg&line=4&uniqifier=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyPcRVcuk6XU"
      },
      "outputs": [],
      "source": [
        "ht_gb = RandomizedSearchCV(GradientBoostingClassifier(), param_grid, random_state=42, n_iter=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2WsOzXXk9fG"
      },
      "outputs": [],
      "source": [
        "ht_gb.fit(X_train_scaled, y_train_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ducXkVimlDlQ"
      },
      "outputs": [],
      "source": [
        "ht_gb.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6qpuPjq1thS"
      },
      "source": [
        "Ein bisschen nochmal verbessert. Ist i.O."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-WYOPnMlGH4"
      },
      "outputs": [],
      "source": [
        "evaluate(ht_gb, X_test_scaled, y_test_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YaRmO_f_Fv-"
      },
      "source": [
        "# **XGBoost**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcJXnEMG28mI"
      },
      "source": [
        "Das XGBoost-Modell ist eine spezielle Implementierung der Ensemble-Methode 'GradientBoosting', die sehr viele Etnscheidungsbäume trainiert, schrittweise miteinander kombiniert und durch Fehlerbeseitung der hinzugefügten Modelle optimiert. XGBoost steht dabei für 'Extreme Gradient Boosting', das optimierte Trainingsalgorithmen verwendet, Gradientenabstieg und Regularisierungstechniken kombiniert, um Overfitting zu reduzieren und ein leistungsstarkes, effizientes Gradient Bossting Modell zu bieten.\n",
        "XGBoost hat sich daher besonders für Klassifikationen im Bereich Data Science bewährt, in dem große Datenmengen verarbeitet werden.\n",
        "\n",
        "Ein Vorteil des XGBoost-Modells ist, dass im Gegensatz zu anderen Gradient Boosting-Implementierungen Regularisierungstechniken (L1- und L2-Regularisierung) integriert sind, sodass Overfitting reduziert und die Modellkomplexität gesteuert werden kann. Außerdem kann das Modell durch viele Parameter flexibel gestaltet werden und sich so an das Problem anpassen. Zuletzt punktet XGBoost auch mit guter Akalierbarkeit, was sich besonders für große Datensätze und viele Features - und somit für unseren Fall - gut eignet.\n",
        "\n",
        "XGBoost performt meist sehr gut und erzielt eine hohe Accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Für die Implementierung importieren wir den XGBClassifier aus dem package 'xgboost' und definieren die Parameter 'objective', 'alpha' (Hyperparameter für die Stärke der L1-Regularisierung) und lambda (L2-Regularisierung). Danach kann das Modell trainiert und als Entscheidungsbaum geplottet werden."
      ],
      "metadata": {
        "id": "_dqTnTBBO3fv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRDO86422jBk"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "params = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'alpha': 0.5,    # L1-Regularisierung\n",
        "    'lambda': 0.5    # L2-Regularisierung\n",
        "}\n",
        "\n",
        "xgb = XGBClassifier(**params)\n",
        "xgb.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "plot_tree(xgb)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-xHFkHP0XJe"
      },
      "source": [
        "alpha (L1-Regularisierung): Der Hyperparameter alpha steuert die Stärke der L1-Regularisierung. Ein höherer Wert von alpha führt zu einer stärkeren Regularisierung und reduziert die Anzahl der Merkmale im Modell. Standardmäßig ist alpha=0, was bedeutet, dass keine L1-Regularisierung angewendet wird.\n",
        "\n",
        "lambda (L2-Regularisierung): Der Hyperparameter lambda steuert die Stärke der L2-Regularisierung. Ein höherer Wert von lambda führt zu einer stärkeren Regularisierung und reduziert die Größe der Modellkoeffizienten. Standardmäßig ist lambda=1, was bedeutet, dass eine leichte L2-Regularisierung angewendet wird."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zuletzt schauen wir uns die Evaluationswerte Accuracy und f1-score an. XGBoost performt im Vergleich zu GradientBoosting deutlich besser."
      ],
      "metadata": {
        "id": "ElA73viXPaC9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UiTgx7E_ZJX"
      },
      "outputs": [],
      "source": [
        "evaluate(xgb, X_test_scaled, y_test_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TxPhvZ3Agm1"
      },
      "source": [
        "## **Hyperparameter Tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEUQqs0oAgm1"
      },
      "source": [
        "Wir versuchen nun durch Hyperparameter Tuning die"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFPyz3rPpmBj"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {\n",
        "    'learning_rate': [0.1, 0.01, 0.001],  # Lernrate\n",
        "    'max_depth': [3, 5, 6, 10],  # Maximale Tiefe der Bäume\n",
        "    'n_estimators': [50, 100, 200],  # Anzahl der Bäume\n",
        "    #'subsample': [0.5, 0.8, 1.0],  # Stichprobengröße für jeden Baum\n",
        "    'min_child_weight': [1, 5, 10, 100],  # Mindestgewicht der Beispiele in einem Blatt\n",
        "    'lambda' : [1,0.5],\n",
        "    'alpha': [0.5,0],\n",
        "    'seed': [42]\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV initialisieren\n",
        "ht_xgb = RandomizedSearchCV(estimator=xgb, param_distributions=param_grid, cv=5)\n",
        "\n",
        "# Hyperparameter-Tuning durchführen\n",
        "ht_xgb.fit(X_train_scaled, y_train_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAjPwVcvAgm2"
      },
      "outputs": [],
      "source": [
        "ht_xgb.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zp1buch9UQT"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT NOTE: hyperparameter tuning with cv can lead to slightly worse results.\n",
        "# This is fine, because the generalization error of the model won't increase through cv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ChWZ7CWAgm2"
      },
      "outputs": [],
      "source": [
        "evaluate(ht_xgb, X_test_scaled, y_test_scaled)"
      ]
    }
  ]
}