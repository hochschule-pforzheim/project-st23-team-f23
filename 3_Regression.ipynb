{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hochschule-pforzheim/project-st23-team-f23/blob/main/3_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dateien ablegen, um sie in Sitzungsspeicher hochzuladen\n",
        "Laufwerk\n",
        "83.52 GB verfügbar\n",
        "Datensatz vorbereiten - Dummies, Skalen\n",
        "Im ersten Schritt wird der Datensatz aus dem ersten Notebook unter dem Namen 'maindf' geladen. Hier muss der Datensatz \"1_Cleansing_Join\" aus dem vorherigen Notebook reingeladen werden."
      ],
      "metadata": {
        "id": "UMZOzgX4BHs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "maindf = pd.read_csv(\"drive/MyDrive/Colab Notebooks/2_Data_for_Modeltraining.csv\")\n",
        "#maindf = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AI & ML/Projekt/1_Cleansing_Join_final.csv')"
      ],
      "metadata": {
        "id": "X6XYN9neBOjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYCdd9if3sTX"
      },
      "source": [
        "# Model Training & Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-MA9Xhr4BLl"
      },
      "source": [
        "And split it into train and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fnjd34TEbsBV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToLX1uco6FIz"
      },
      "source": [
        "We will test the following models and compare their performance:\n",
        "\n",
        "*   Linear Regression (with and without regularization)\n",
        "*   Support Vector Mechine (SVM)\n",
        "*   Decision Tree\n",
        "*   Gradient Boosting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-clOeqGcPU6"
      },
      "source": [
        "Before we do that, let's again implement a small `evaluate` function for measuring the performance of our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdLpRhdtcUQ1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "def evaluate(reg, X_test, y_test):\n",
        "  pred = reg.predict(X_test)\n",
        "  print('R2:', r2_score(y_test, pred))\n",
        "  print('MAE:', mean_absolute_error(y_test, pred))\n",
        "  print('MSE:', mean_squared_error(y_test, pred))\n",
        "  print('RMSE:', mean_squared_error(y_test, pred, squared=False))\n",
        "  print('MAPE:', mean_absolute_percentage_error(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKAPZ1cVagV5"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NYvNxFp8zcC"
      },
      "source": [
        "Next, we take a look at a support vector machine (SVM) model. Normally, SVMs are used for classification but they can also be applied to regression problems.\n",
        "\n",
        "`sklearn` provides a SVM regression implementation which we can easily use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmmzWrgrZytj"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsfTnomganCp"
      },
      "outputs": [],
      "source": [
        "lsvr = make_pipeline(StandardScaler(),\n",
        "                     LinearSVR(random_state=42, tol=1e-5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29hUT1AMapVn"
      },
      "outputs": [],
      "source": [
        "lsvr.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pmRshGyasyp"
      },
      "outputs": [],
      "source": [
        "evaluate(lsvr, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJwRQMIEPUyw"
      },
      "source": [
        "The SVM achieves a pretty good result - significantly better than the regression models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FSx2a1Tmkqz"
      },
      "outputs": [],
      "source": [
        "y_lsvr_predict = lsvr.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYQU5zYspx5z"
      },
      "source": [
        "### **Hyperparameter Tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZKT2XixsMTs"
      },
      "source": [
        "C: da LinearSVC: andere Parameter wie 'learning_rate' nicht valid.\n",
        "\n",
        "Die Fehlermeldung \"TypeError: Parameter grid for parameter 'C' is not iterable or a distribution (value=1)\" deutet darauf hin, dass es ein Problem mit dem angegebenen Parameterraum für den Parameter 'C' gibt. Der Wert '1' allein ist nicht als Parameterraum gültig.\n",
        "\n",
        "Um das Problem zu beheben, kannst du entweder einen spezifischen Wert oder einen Parameterraum für 'C' angeben. Hier sind zwei Möglichkeiten:\n",
        "\n",
        "from scipy.stats import uniform\n",
        "\n",
        "param_grid = {\n",
        "    'C': uniform(loc=0.1, scale=10),\n",
        "\n",
        "    Die Wahl zwischen der Verwendung spezifischer Werte und der Verwendung eines Parameterraums hat Vor- und Nachteile, die ich im Folgenden erläutern werde:\n",
        "\n",
        "Option 1: Spezifische Werte für 'C':\n",
        "Vorteile:\n",
        "\n",
        "Einfachheit: Die Verwendung spezifischer Werte ist einfach und erfordert keine weiteren Berechnungen.\n",
        "Kontrolle: Du hast direkte Kontrolle über die ausgewählten Werte für 'C' und kannst gezielt bestimmte Werte testen.\n",
        "Nachteile:\n",
        "\n",
        "Möglicher Informationsverlust: Es besteht die Möglichkeit, dass der optimale Wert für 'C' nicht in den spezifischen Werten enthalten ist. Dies kann dazu führen, dass das Tuning nicht das bestmögliche Ergebnis erzielt.\n",
        "Option 2: Parameterraum für 'C':\n",
        "Vorteile:\n",
        "\n",
        "Flexibilität: Durch die Verwendung eines Parameterraums kannst du einen breiteren Bereich von 'C'-Werten abdecken und somit eine größere Chance haben, den optimalen Wert zu finden.\n",
        "Automatisierte Suche: Der Hyperparameter-Tuning-Algorithmus (z. B. RandomizedSearchCV) kann den Parameterraum automatisch durchsuchen und die besten Kombinationen finden.\n",
        "Nachteile:\n",
        "\n",
        "Komplexität: Die Definition eines Parameterraums erfordert möglicherweise zusätzliche Überlegungen und Kenntnisse über die Wertebereiche von 'C'. Die Auswahl eines geeigneten Parameterraums kann eine gewisse Erfahrung oder Experimentation erfordern.\n",
        "Berechnungsaufwand: Ein breiterer Parameterraum kann zu einer erhöhten Rechenzeit führen, da der Hyperparameter-Tuning-Algorithmus mehr Kombinationen ausprobieren muss.\n",
        "Es ist wichtig zu beachten, dass die Vor- und Nachteile von Optionen wie spezifischen Werten oder Parameterräumen stark vom Anwendungsfall und den Eigenschaften des Modells abhängen können. Es empfiehlt sich, verschiedene Optionen auszuprobieren und die Auswirkungen auf die Leistung des Modells zu beobachten, um die besten Hyperparameter-Tuning-Strategien zu ermitteln.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owJHJYT8pwWd"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import loguniform\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'class_weight': [None, 'balanced'],\n",
        "    'dual': [True, False],\n",
        "    'fit_intercept': [True, False],\n",
        "    'intercept_scaling': loguniform(0.1, 10),\n",
        "    'loss': ['hinge', 'squared_hinge'],\n",
        "    'max_iter': [100, 500, 1000],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'tol': loguniform(0.0001, 0.1),\n",
        "    'verbose': [0, 1]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok21JGPjpwWe"
      },
      "outputs": [],
      "source": [
        "ht_lsvr = RandomizedSearchCV(lsvr, param_grid, random_state=42, n_iter=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMRIg-zqpwWe"
      },
      "outputs": [],
      "source": [
        "ht_svc.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmS_PT2Oum7D"
      },
      "source": [
        "Trotz Hyperparameter Tuning richtig schlechte Accuracy => nicht geeignet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "728TuCjapwWe"
      },
      "outputs": [],
      "source": [
        "evaluate(ht_svc, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONo9BImmellg"
      },
      "source": [
        "## Gradient Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V__ZZVBMVloW"
      },
      "source": [
        "Let's move on to a gradient boosting model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9tdqcRBem1e"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-kA4LUJeo7o"
      },
      "outputs": [],
      "source": [
        "gb = make_pipeline(StandardScaler(),\n",
        "                   GradientBoostingRegressor(random_state=42))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUY3lDgeesJf"
      },
      "outputs": [],
      "source": [
        "gb.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rp6mpapYetWK"
      },
      "outputs": [],
      "source": [
        "evaluate(gb, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FNiXef9cUv7"
      },
      "source": [
        "It achieves a very high performance!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKBHkNGU2GLM"
      },
      "outputs": [],
      "source": [
        "from sklearn.dummy import DummyRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYVzZNPC2HbW"
      },
      "outputs": [],
      "source": [
        "dummy = DummyRegressor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDawXTNd2KYv"
      },
      "outputs": [],
      "source": [
        "dummy.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXCIBcgu2Osx"
      },
      "outputs": [],
      "source": [
        "evaluate(dummy, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC9lxLago_sS"
      },
      "source": [
        "Instead of relying on a visual plot to determine the most important features, we can also utilize the `feature_importances_` attribute on the trained sklearn estimator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jly0hnhtgjmJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "feature_importance = gb.feature_importances_\n",
        "\n",
        "# get indices sorted by importance\n",
        "sorted_idx = np.argsort(feature_importance)\n",
        "\n",
        "# generate range from 0 to the number of features\n",
        "pos = np.arange(sorted_idx.shape[0])\n",
        "\n",
        "# 'pos' acts as our value for the 'y' axis\n",
        "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
        "\n",
        "plt.yticks(pos, np.array(X.columns)[sorted_idx])\n",
        "plt.title('Feature Importance (MDI)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6TbL_gnq_6t"
      },
      "source": [
        "As expected, the feature importance we find here is similar to the one we found visually in the decision tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVQYRA0-V0mW"
      },
      "source": [
        "The model's performance appears to be quite promising so let's try to improve that further. For this purpose, we will perform some basic hyperparameter optimizations in a next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bm3UDb8WcrJ"
      },
      "source": [
        "### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZrDKlSnWeZ_"
      },
      "source": [
        "We'll again be using the random search approach. As you can see from the `CV` suffix this is combined with cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSztS17Daugu"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6UzfogPWvYu"
      },
      "source": [
        "Again, we first have to define our hyperparameter grid:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qW3YCXi3cYP5"
      },
      "outputs": [],
      "source": [
        "# the algorithm will try out random values from the lists we provide\n",
        "grid = {'learning_rate': [0.15, 0.1, 0.05, 0.01, 0.005, 0.001],\n",
        "        'n_estimators': [100, 250, 500, 750, 1000, 1250, 1500, 1750],\n",
        "        'max_depth': [2, 3, 4, 5, 6, 7]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y_AInykW7Dy"
      },
      "source": [
        "We can now start the tuning process - limited to 100 iterations (100 different hyperparameter combinations). Note that this process can take a while because sklearn will fit the model 5 times (folds) for each iteration resulting in 500 total fits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "houu11J7cNZp"
      },
      "outputs": [],
      "source": [
        "# setting n_jobs=-1 will ensure that sklearn uses all available cpu cores\n",
        "# takes a loooong time (reduce n_iter to make it faster)\n",
        "optimized_gb = RandomizedSearchCV(pipeline, param_grid, n_iter=100, n_jobs=-1, random_state=42, cv=5, verbose=1)\n",
        "optimized_gb.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt-lCUjuX1O-"
      },
      "source": [
        "Now that the tuning is done, we can retrieve the best hyperparamter combination as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHnRjUoPckUN"
      },
      "outputs": [],
      "source": [
        "optimized_gb.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q83H9_tnX6UL"
      },
      "source": [
        "And evaluate the model trained using those parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4MQq4Hkck-j"
      },
      "outputs": [],
      "source": [
        "evaluate(optimized_gb, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJqMGC8qYBbB"
      },
      "source": [
        "We improved the performance of our model slightly - not a huge improvement, but better than nothing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVAP0maZgThw"
      },
      "source": [
        "The gradient boosting model (in fact, all ML models) might suffer from overfitting where they memorize the data instead of learning the underlying patterns. One way of detecting this for a gradient boosting model is plotting the train and test error against the number of iterations (i.e. how long the model was trained for)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create this plot here:"
      ],
      "metadata": {
        "id": "IwPGq5QVUTvB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCLxG1cgnQC7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-Z1W38qnRA0"
      },
      "outputs": [],
      "source": [
        "# get best xgb model (after hyperparam tuning)\n",
        "best_gb = optimized_xgb.best_estimator_\n",
        "# staged_predict returns the error after each stage in the model\n",
        "predictions = best_gb.staged_predict(X_test)\n",
        "# how many estimators were used for the model\n",
        "n_estimators = optimized_gb.best_params_['n_estimators']\n",
        "\n",
        "# calculate deviance (error) for all examples in test set\n",
        "test_score = np.zeros((n_estimators,), dtype=np.float64)\n",
        "for i, y_pred in enumerate(predictions):\n",
        "    test_score[i] = best_gb.loss_(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_oQVqjCnf7g"
      },
      "source": [
        "Now we can plot the test loss against the training loss which is already stored in the `train_score_` attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8qLDGiegTF3"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6, 6))\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.title('Deviance')\n",
        "plt.plot(np.arange(n_estimators) + 1, best_xgb.train_score_, 'b-',\n",
        "         label='Training Set Deviance')\n",
        "plt.plot(np.arange(n_estimators) + 1, test_score, 'r-',\n",
        "         label='Test Set Deviance')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlabel('Boosting Iterations')\n",
        "plt.ylabel('Deviance')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **XGB Regression**"
      ],
      "metadata": {
        "id": "AGczG6aCQdAL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRDO86422jBk"
      },
      "outputs": [],
      "source": [
        "# plot decision tree\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBRegressor\n",
        "from xgboost import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# fit model no training data\n",
        "xgb = make_pipeline(StandardScaler(),\n",
        "                    XGBRegressor())\n",
        "xgb.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(xgb, X_train, y_train)"
      ],
      "metadata": {
        "id": "_UiTgx7E_ZJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ik_tNNra38b6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08wf9tn838b8"
      },
      "outputs": [],
      "source": [
        "# the algorithm will try out random values from the lists we provide\n",
        "grid = {\n",
        "    'xgbregressor__n_estimators': [100, 250, 500, 750, 1000, 1250, 1500, 1750],\n",
        "    'xgbregressor__max_depth': [2, 3, 4, 5, 6, 7],\n",
        "    'xgbregressor__learning_rate': [0.15, 0.1, 0.05, 0.01, 0.005, 0.001]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ljm2No_38b8"
      },
      "source": [
        "We can now start the tuning process - limited to 100 iterations (100 different hyperparameter combinations). Note that this process can take a while because sklearn will fit the model 5 times (folds) for each iteration resulting in 500 total fits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw39OIsS38b9"
      },
      "outputs": [],
      "source": [
        "# setting n_jobs=-1 will ensure that sklearn uses all available cpu cores\n",
        "# takes a loooong time (reduce n_iter to make it faster)\n",
        "optimized_xgb = RandomizedSearchCV(xgb, grid, n_iter=100, n_jobs=-1, random_state=42, verbose=3)\n",
        "optimized_xgb.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNCbsm9H38b9"
      },
      "source": [
        "Now that the tuning is done, we can retrieve the best hyperparamter combination as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNzKLCPQ38b9"
      },
      "outputs": [],
      "source": [
        "optimized_xgb.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6PjIqa538b9"
      },
      "source": [
        "And evaluate the model trained using those parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMt60ev038b9"
      },
      "outputs": [],
      "source": [
        "evaluate(optimized_xgb, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsEVG-aynpyV"
      },
      "source": [
        "We can see that there is not a big difference between training and test set deviance which means that our model is not suffering from overfitting (at least not heavily). However, we might choose to stop training after around iteration 20 as the additional iterations only contribute very little to the overall model performance. That being said, we are only looking at an individual hyperparam here so this might not be the true optimal parameter value."
      ]
    }
  ]
}